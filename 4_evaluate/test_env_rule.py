import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd

class TradingEnv(gym.Env):
    """
    [ìˆ˜ì •ë¨]
    - ì •ê·œí™” í†µê³„(means/stds)ë¥¼ ë‚´ë¶€ì—ì„œ ê³„ì‚°í•˜ì§€ ì•Šê³ ,
      ì™¸ë¶€ì—ì„œ íŒŒë¼ë¯¸í„°ë¡œ ì£¼ì…ë°›ì•„ ë°ì´í„° ìœ ì¶œ(Lookahead Bias)ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    - [ë°±í…ŒìŠ¤íŠ¸ìš© ìˆ˜ì •] step()ì˜ info ë”•ì…”ë„ˆë¦¬ì—
      'portfolio_value', 'date', 'traded'ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    metadata = {"render.modes": ["human"]}

    # âœ… [ìˆ˜ì •] __init__ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½: obs_meansì™€ obs_stdsë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ìŒ
    def __init__(self, df: pd.DataFrame, 
                 obs_means: pd.Series, 
                 obs_stds: pd.Series,
                 window_size: int = 10, 
                 episode_length: int = 96,
                 trailing_stop_pct: float = 0.001,
                 trade_ratio: float = 0.5):
        
        super().__init__()
        
        # â—ï¸ ì¤‘ìš”: 'datetime' ì»¬ëŸ¼ì´ dfì— í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        #    ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŒ…ì— ì´ ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if 'datetime' not in df.columns:
            raise ValueError("DataFrame 'df' must contain a 'datetime' column for backtesting.")
            
        self.df = df.reset_index(drop=True).copy()
        
        self.window_size = window_size
        self.episode_length = episode_length

        # --- Trading Parameters ---
        self.initial_balance = 300_000.0
        self.min_trade_krw = 5000.0
        self.fee = 0.0005
        self.trade_ratio = trade_ratio
        self.trailing_stop_pct = trailing_stop_pct
        
        # --- Rewards and Penalties ---
        self.reward_scaling = 100.0
        self.profit_bonus = 5.0
        self.loss_penalty = -5.0
        self.shaping_scaling = 50.0

        # --- Observation and Action Spaces ---
        
        # Lasso (C=0.01)ë¡œ ì„ íƒëœ 6ê°œì˜ í•µì‹¬ ì§€í‘œ
        self.obs_cols = [
            '30_to_60_Close_ratio', 
            '60_OBV', 
            'day_of_week', 
            '30_ATR', 
            '30_Keltner_lband', 
            '60_ADX'
        ]  
        
        self.portfolio_info_len = 5 
        num_features = len(self.obs_cols) + self.portfolio_info_len
        
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(self.window_size, num_features), 
            dtype=np.float32
        )
        self.action_space = spaces.Discrete(2)

        # âœ… [ìˆ˜ì •] ì •ê·œí™” í†µê³„ë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ìŒ
        self.obs_means = obs_means
        self.obs_stds = obs_stds
        
        self.highest_price_since_buy = 0.0
        self.reset()

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        # 60ë¶„ë´‰ ì§€í‘œ(60_OBV, 60_ADX) ë“±ì„ ìœ„í•œ ì•ˆì „ ë§ˆì§„ (120ì´ë©´ ì¶©ë¶„)
        self.safe_start_margin = self.window_size + 120 
        
        max_start = len(self.df) - self.episode_length - self.safe_start_margin
        
        # ë°±í…ŒìŠ¤íŠ¸ ëª¨ë“œ(episode_lengthê°€ ì „ì²´ ê¸¸ì´ì¼ ê²½ìš°) max_startê°€ 0ì´ ë˜ì–´
        # í•­ìƒ 0ì—ì„œ ì‹œì‘í•˜ê²Œ ë©ë‹ˆë‹¤.
        self.start_idx = np.random.randint(0, max_start + 1) if max_start > 0 else 0
        self.current_step = self.start_idx + self.safe_start_margin
        
        self.step_idx = 0
        self.balance = float(self.initial_balance)
        self.coin_holdings = 0.0
        self.avg_buy_price = 0.0
        self.highest_price_since_buy = 0.0

        self.previous_price = self.df.loc[self.current_step, 'Close']
        
        return self._get_obs(), {}

    def _get_obs(self):
        start = self.current_step - self.window_size + 1
        end = self.current_step + 1
        
        if start < 0:
            start = 0
            
        window_df = self.df.iloc[start:end]
        
        # âœ… [ìˆ˜ì •] ì •ê·œí™” ì‹œ (ë¯¸ë˜ ë°ì´í„°ê°€ ì•„ë‹Œ) ì£¼ì…ë°›ì€ í†µê³„(self.obs_means) ì‚¬ìš©
        norm_obs_window = (window_df[self.obs_cols] - self.obs_means) / self.obs_stds
        
        current_price = self.df.loc[self.current_step, 'Close']
        is_holding = 1.0 if self.coin_holdings > 0 else 0.0
        unrealized_pnl = (current_price - self.avg_buy_price) / (self.avg_buy_price + 1e-9) if is_holding else 0.0
        
        portfolio_info = np.array([
            (self.balance - self.initial_balance) / (self.initial_balance * 0.5),
            (self.coin_holdings * current_price) / self.initial_balance,
            is_holding, 
            unrealized_pnl, 
            self.step_idx / max(1, self.episode_length)
        ])
        
        portfolio_info_tiled = np.tile(portfolio_info, (self.window_size, 1))
        
        current_window_len = len(norm_obs_window)
        if current_window_len < self.window_size:
            padding = np.zeros((self.window_size - current_window_len, len(self.obs_cols)))
            norm_obs_window_values = np.concatenate([padding, norm_obs_window.values], axis=0)
        else:
            norm_obs_window_values = norm_obs_window.values

        obs_array = np.concatenate([norm_obs_window_values, portfolio_info_tiled], axis=1)
        return obs_array.astype(np.float32)

    
    def _calculate_reward(self, realized_pnl, is_stop_loss, is_take_profit):
        reward = 0.0
        if realized_pnl != 0:
            reward += realized_pnl * self.reward_scaling
            if is_take_profit:
                reward += self.profit_bonus
            elif is_stop_loss:
                reward += self.loss_penalty
        return float(np.clip(reward, -20.0, 20.0))

    def step(self, action):
        current_price = self.df.loc[self.current_step, 'Close']
        is_holding = self.coin_holdings > 0
        is_stop_loss, is_take_profit, traded = False, False, False
        realized_pnl = 0.0
        
        # âœ… --- [ì¶”ê°€] ë³´ìƒ ì‰ì´í•‘ (Dense Reward) ---
        shaping_reward = 0.0
        price_change_pct = (current_price - self.previous_price) / (self.previous_price + 1e-9)

        if is_holding:
            shaping_reward = price_change_pct
        else:
            shaping_reward = -price_change_pct
        shaping_reward *= self.shaping_scaling
        # -----------------------------------------------

        # 1. ìë™ ë§¤ë„ ë¡œì§ (íŠ¸ë ˆì¼ë§ ìŠ¤íƒ‘)
        if is_holding:
            self.highest_price_since_buy = max(self.highest_price_since_buy, current_price)
            trailing_stop_price = self.highest_price_since_buy * (1 - self.trailing_stop_pct)
            
            if current_price <= trailing_stop_price:
                _, r_pnl = self._sell(self.coin_holdings, current_price)
                realized_pnl += r_pnl
                traded = True
                
                if realized_pnl > 0:
                    is_take_profit = True
                else:
                    is_stop_loss = True

        # 2. ì—ì´ì „íŠ¸ì˜ ë§¤ìˆ˜ ë¡œì§
        elif not is_holding and action == 1:
            cost_to_spend = self.balance * self.trade_ratio
            buy_qty = cost_to_spend / current_price if current_price > 0 else 0
            cost = self._buy(buy_qty, current_price)
            if cost > 0:
                traded = True

        # --- Calculate reward and move to the next step ---
        realized_reward = self._calculate_reward(realized_pnl, is_stop_loss, is_take_profit)
        step_reward = realized_reward + shaping_reward # ğŸ‘ˆ ë‘ ë³´ìƒì„ í•©ì‚°

        self.previous_price = current_price
        
        self.current_step += 1
        self.step_idx += 1
        
        terminated = self.current_step >= (len(self.df) - 1)
        truncated = self.step_idx >= self.episode_length
        
        # â­ï¸â­ï¸â­ï¸ [ë°±í…ŒìŠ¤íŠ¸ìš© í•µì‹¬ ìˆ˜ì •] â­ï¸â­ï¸â­ï¸
        # quantstatsê°€ ìš”êµ¬í•˜ëŠ” 'í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜'ì™€ 'ë‚ ì§œ'ë¥¼ infoì— ë‹´ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
        # 'datetime' ì»¬ëŸ¼ì´ self.dfì— ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
        
        current_total_asset = self.balance + self.coin_holdings * current_price
        
        info = {
            # 1. í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ (ì´ì „ ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜ì„ ìœ„í•´ 'portfolio_value' ì‚¬ìš©)
            'portfolio_value': current_total_asset,
            
            # 2. í˜„ì¬ ë‚ ì§œ/ì‹œê°„ (dfì—ì„œ 'datetime' ì»¬ëŸ¼ ì¡°íšŒ)
            'date': self.df.loc[self.current_step - 1, 'datetime'], # (stepì´ ì¦ê°€í–ˆìœ¼ë¯€ë¡œ -1)
            
            # 3. ê±°ë˜ ë°œìƒ ì—¬ë¶€ (ì´ ê±°ë˜ íšŸìˆ˜ ì¹´ìš´íŠ¸ìš©)
            'traded': traded,
            
            # (ê¸°íƒ€ ì •ë³´)
            'realized_pnl_pct': realized_pnl,
            'is_holding': is_holding
        }
        
        # â­ï¸â­ï¸â­ï¸ [ìˆ˜ì • ë] â­ï¸â­ï¸â­ï¸

        if terminated or truncated:
            obs = self._get_obs() # ë§ˆì§€ë§‰ obs (ì‚¬ìš©ë˜ì§€ëŠ” ì•ŠìŒ)
        else:
            obs = self._get_obs()
            
        return obs, step_reward, terminated, truncated, info

    def _buy(self, qty, price):
        if qty <= 0 or price <= 0: return 0.0
        cost = qty * price * (1 + self.fee)
        if cost < self.min_trade_krw or cost > self.balance: return 0.0
        
        self.avg_buy_price = price
        self.coin_holdings = qty
        self.balance -= cost
        self.highest_price_since_buy = price
        return cost

    def _sell(self, qty, price):
        if qty <= 0 or price <= 0 or self.coin_holdings <= 0: return 0.0, 0.0
        
        revenue = qty * price * (1 - self.fee)
        realized_pnl = (price - self.avg_buy_price) / (self.avg_buy_price + 1e-9)
        
        self.coin_holdings = 0.0
        self.avg_buy_price = 0.0
        self.balance += revenue
        self.highest_price_since_buy = 0.0
        return revenue, realized_pnl

    def render(self, mode="human"):
        # renderëŠ” ë°±í…ŒìŠ¤íŠ¸ ì‹œ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìƒëµ ê°€ëŠ¥
        pass