import os
import pandas as pd
import numpy as np
import pickle # âœ… [ì¶”ê°€] í†µê³„ ì €ì¥ì„ ìœ„í•´ pickle ì‚¬ìš©
from stable_baselines3.common.vec_env import DummyVecEnv
# âš ï¸ [ì‚­ì œ] VecNormalizeëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement
from sb3_contrib import RecurrentPPO
from env_rule import TradingEnv # ğŸ‘† ìœ„ì—ì„œ ìˆ˜ì •í•œ env.pyë¥¼ ì„í¬íŠ¸

def train_agent():
    """ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í•™ìŠµ ë° í‰ê°€ íŒŒì´í”„ë¼ì¸"""

    # ===== 1. ì„¤ì • (Configuration) =====
    # --- íŒŒì¼ ê²½ë¡œ ---
    data_path = "0_data/btc_updated_6indi.csv"
    log_dir = "./0_logs/model_log/"
    best_model_save_path = "./1_model/best_model_auto"
    final_model_save_path = "./1_model/final_model_auto"
    
    # âœ… [ìˆ˜ì •] VecNormalize í†µê³„ ëŒ€ì‹ , ìˆ˜ë™ ê³„ì‚°í•œ í†µê³„ íŒŒì¼ ì €ì¥
    stats_save_path = os.path.join(os.path.dirname(best_model_save_path), "obs_stats_btc_rule.pkl")

    # --- í•™ìŠµ íŒŒë¼ë¯¸í„° ---
    total_timesteps = 1_000_000  # í•„ìš”ì‹œ 500_000 ì´ìƒìœ¼ë¡œ ëŠ˜ë¦¬ê¸°
    episode_length = 48       # 15ë¶„ë´‰ * 96 = 24ì‹œê°„
    window_size = 10          # LSTMì´ ë³¼ ê³¼ê±° ë°ì´í„°ì˜ ê¸¸ì´

    # --- ë°ì´í„° ë¶„ë¦¬ ---
    train_test_split_ratio = 0.8
    
    # --- ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
    policy_kwargs = dict(
        lstm_hidden_size=256,
        net_arch=dict(pi=[256], vf=[256]) 
    )
    model_params = {
        'policy': "MlpLstmPolicy",
        'learning_rate': 3e-5,
        'n_steps': 1024, 
        'batch_size': 64,
        'ent_coef': 0.01, 
        'verbose': 1,
        'tensorboard_log': log_dir,
        'policy_kwargs': policy_kwargs
    }
    
    # âœ… [ì¶”ê°€] Lasso (C=0.01)ë¡œ ì„ íƒëœ 6ê°œì˜ í•µì‹¬ ì§€í‘œ
    OBS_COLS = [
        '30_to_60_Close_ratio', 
        '60_OBV', 
        'day_of_week', 
        '30_ATR', 
        '30_Keltner_lband', 
        '60_ADX'
    ]

    # ===== 2. ë°ì´í„° ë¡œë“œ ë° ë¶„ë¦¬ =====
    df = pd.read_csv(data_path, parse_dates=['datetime'], index_col='datetime')
    
    # âœ… [ìˆ˜ì •] ê²°ì¸¡ì¹˜ ì œê±° (ì§€í‘œ ê³„ì‚° ì‹œ ë°œìƒí•œ ì´ˆê¸° NaN ì œê±°)
    df = df.dropna(subset=OBS_COLS + ['Close']) # obs_colsì™€ ê°€ê²© ë°ì´í„°ê°€ ëª¨ë‘ ìˆì–´ì•¼ í•¨
    df = df.reset_index() # TradingEnvëŠ” reset_indexëœ dfë¥¼ ì‚¬ìš©í•¨

    split_point = int(len(df) * train_test_split_ratio)
    
    train_df = df[:split_point]
    eval_df = df[split_point:]

    print(f"í•™ìŠµ ë°ì´í„° í¬ê¸°: {len(train_df)}")
    print(f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {len(eval_df)}")

    # ===== 3. âœ… [ìˆ˜ì •] ì •ê·œí™” í†µê³„ ê³„ì‚° (í›ˆë ¨ ë°ì´í„°ë¡œë§Œ!) =====
    print("í›ˆë ¨ ë°ì´í„°(train_df)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...")
    obs_means = train_df[OBS_COLS].mean()
    obs_stds = train_df[OBS_COLS].std().replace(0, 1.0) # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    
    # ê³„ì‚°ëœ í†µê³„ ì €ì¥ (ë‚˜ì¤‘ì— ë°±í…ŒìŠ¤íŠ¸/ì‹¤ì „ë§¤ë§¤ì—ì„œ ë¡œë“œí•´ì•¼ í•¨)
    stats = {'means': obs_means, 'stds': obs_stds}
    with open(stats_save_path, 'wb') as f:
        pickle.dump(stats, f)
    print(f"ì •ê·œí™” í†µê³„ê°€ '{stats_save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ===== 4. âœ… [ìˆ˜ì •] í™˜ê²½ ìƒì„± (VecNormalize ì œê±°) =====
    # --- í•™ìŠµ í™˜ê²½ ---
    # ì£¼ì…ë°›ì€ í†µê³„(obs_means, obs_stds)ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ëŒë‹¤ í•¨ìˆ˜ ìˆ˜ì •
    train_env_fn = lambda: TradingEnv(
        train_df, 
        obs_means=obs_means, 
        obs_stds=obs_stds, 
        window_size=window_size, 
        episode_length=episode_length
    )
    # VecNormalize ì œê±°
    train_env = DummyVecEnv([train_env_fn]) 
    
    # --- ê²€ì¦ í™˜ê²½ ---
    # âš ï¸ [ì¤‘ìš”] ê²€ì¦ í™˜ê²½ì—ë„ 'í›ˆë ¨ ë°ì´í„°ì˜ í†µê³„'ë¥¼ ë™ì¼í•˜ê²Œ ì£¼ì…
    eval_env_fn = lambda: TradingEnv(
        eval_df, 
        obs_means=obs_means, 
        obs_stds=obs_stds, 
        window_size=window_size, 
        episode_length=len(eval_df) # ê²€ì¦ì€ ì „ì²´ ê¸°ê°„ì„ í•œ ì—í”¼ì†Œë“œë¡œ
    )
    # VecNormalize ì œê±°
    eval_env = DummyVecEnv([eval_env_fn]) 
    
    # âš ï¸ [ì‚­ì œ] VecNormalize ê´€ë ¨ ì½”ë“œ ëª¨ë‘ ì‚­ì œë¨
    print("í™˜ê²½ ìƒì„± ì™„ë£Œ. (VecNormalize ëŒ€ì‹  í™˜ê²½ ë‚´ë¶€ ì •ê·œí™” ì‚¬ìš©)")

    # ===== 5. ì½œë°±(Callback) ì„¤ì • =====
    # (ì½œë°± ì„¤ì •ì€ VecNormalizeì™€ ë¬´ê´€í•˜ë¯€ë¡œ ê¸°ì¡´ê³¼ ë™ì¼)
    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=20, verbose=1)

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=best_model_save_path,
        log_path=log_dir,
        eval_freq=model_params['n_steps'] * 10,
        n_eval_episodes=1, 
        deterministic=True,
        render=False,
        callback_on_new_best=stop_train_callback 
    )

    # ===== 6. ëª¨ë¸ ìƒì„± ë° í•™ìŠµ =====
    model = RecurrentPPO(env=train_env, **model_params)
    
    print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    model.learn(
        total_timesteps=total_timesteps,
        callback=eval_callback 
    )

    # ===== 7. âœ… [ìˆ˜ì •] ìµœì¢… ëª¨ë¸ ì €ì¥ =====
    print("âœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.")
    model.save(final_model_save_path)
    
    # âš ï¸ [ì‚­ì œ] VecNormalize í†µê³„ ì €ì¥ì€ ì´ë¯¸ ìœ„ 3ë²ˆ ì„¹ì…˜ì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ì™„ë£Œë¨
    print(f"ìµœì¢… ëª¨ë¸: '{final_model_save_path}'")
    print(f"í›ˆë ¨ í†µê³„: '{stats_save_path}'")


if __name__ == '__main__': 
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./1_model", exist_ok=True)
    os.makedirs("./0_logs/recurrent_ppo", exist_ok=True)
    os.makedirs("./0_data", exist_ok=True) # data ë””ë ‰í† ë¦¬ë„ ìƒì„± (í˜¹ì‹œ ëª¨ë¥´ë‹ˆ)
    
    train_agent()