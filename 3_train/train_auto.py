import os
import pandas as pd
import numpy as np
import pickle 
from stable_baselines3.common.vec_env import DummyVecEnv
# âš ï¸ [ì‚­ì œ] VecNormalizeëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement
from sb3_contrib import RecurrentPPO
from env_auto import TradingEnv # TradingEnv í´ë˜ìŠ¤ê°€ ìˆëŠ” ëª¨ë“ˆì„ ì„í¬íŠ¸

def train_agent():
    """ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í•™ìŠµ ë° í‰ê°€ íŒŒì´í”„ë¼ì¸"""

    # ===== 1. ì„¤ì • (Configuration) =====
    # --- íŒŒì¼ ê²½ë¡œ ---
    data_path = "0_data/updated.csv"
    log_dir = "./0_logs/recurrent_ppo/" # í…ì„œë³´ë“œ ë¡œê·¸ ê²½ë¡œë¥¼ ëª¨ë¸ ì´ë¦„ì— ë§ê²Œ ìˆ˜ì •
    best_model_save_path = "./1_model/best_model_auto" # ëª¨ë¸ íŒŒì¼ ì´ë¦„ ìˆ˜ì •
    final_model_save_path = "./1_model/final_auto" # ëª¨ë¸ íŒŒì¼ ì´ë¦„ ìˆ˜ì •
    
    # âœ… [ìˆ˜ì •] í†µê³„ íŒŒì¼ ì €ì¥ ê²½ë¡œ
    stats_save_path = os.path.join(os.path.dirname(best_model_save_path), "obs_stats_btc_auto.pkl")

    # --- í•™ìŠµ íŒŒë¼ë¯¸í„° ---
    total_timesteps = 1_000_000 
    episode_length = 48 
    window_size = 10 
    train_test_split_ratio = 0.8
    
    # --- ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
    policy_kwargs = dict(
        lstm_hidden_size=256,
        net_arch=dict(pi=[256], vf=[256]) 
    )
    model_params = {
        'policy': "MlpLstmPolicy",
        'learning_rate': 3e-5,
        'n_steps': 1024, 
        'batch_size': 64,
        'ent_coef': 0.01, 
        'verbose': 1,
        'tensorboard_log': log_dir,
        'policy_kwargs': policy_kwargs
    }
    
    # âœ… [ìˆ˜ì •] (Lasso C=0.01)ë¡œ ì„ íƒëœ 9ê°œ ì§€í‘œ
    OBS_COLS = ['60_BB_Width', 
                '30_VPT', 
                '30_ADI', 
                'day_of_week', 
                '30_OBV', 
                '30_to_60_Close_ratio', 
                '30_BB_High', 
                '30_ATR', 
                '60_ADX']

    # ===== 2. ë°ì´í„° ë¡œë“œ ë° ë¶„ë¦¬ =====
    df = pd.read_csv(data_path, parse_dates=['datetime'], index_col='datetime')
    
    # âœ… [ìˆ˜ì •] ê²°ì¸¡ì¹˜ ì œê±°
    df = df.dropna(subset=OBS_COLS + ['Close']) 
    df = df.reset_index(drop=True) # TradingEnvëŠ” reset_indexëœ dfë¥¼ ì‚¬ìš©í•¨

    split_point = int(len(df) * train_test_split_ratio)
    
    train_df = df[:split_point].copy() # ë¶ˆí•„ìš”í•œ SettingWithCopyWarning ë°©ì§€ë¥¼ ìœ„í•´ copy() ì¶”ê°€
    eval_df = df[split_point:].copy() # ë¶ˆí•„ìš”í•œ SettingWithCopyWarning ë°©ì§€ë¥¼ ìœ„í•´ copy() ì¶”ê°€

    print(f"í•™ìŠµ ë°ì´í„° í¬ê¸°: {len(train_df)}")
    print(f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {len(eval_df)}")

    # ===== 3. âœ… [ìˆ˜ì •] ì •ê·œí™” í†µê³„ ê³„ì‚° ë° ì €ì¥ (í›ˆë ¨ ë°ì´í„°ë¡œë§Œ!) =====
    print("í›ˆë ¨ ë°ì´í„°(train_df)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤...")
    obs_means = train_df[OBS_COLS].mean()
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³ , Series í˜•íƒœë¡œ ìœ ì§€
    obs_stds = train_df[OBS_COLS].std().replace(0, 1e-6) 
    
    # ê³„ì‚°ëœ í†µê³„ ì €ì¥ 
    stats = {'means': obs_means.to_dict(), 'stds': obs_stds.to_dict()} # ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥í•˜ì—¬ ë¡œë“œ ì‹œ í¸ë¦¬í•˜ê²Œ í•¨
    with open(stats_save_path, 'wb') as f:
        pickle.dump(stats, f)
    print(f"ì •ê·œí™” í†µê³„ê°€ '{stats_save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ===== 4. âœ… [ìˆ˜ì •] í™˜ê²½ ìƒì„± (VecNormalize ì œê±° ë° í†µê³„ ì£¼ì…) =====
    
    # --- í•™ìŠµ í™˜ê²½ ---
    # DummyVecEnvë§Œ ì‚¬ìš©í•˜ë©°, ê³„ì‚°ëœ í†µê³„ë¥¼ TradingEnvì— ì§ì ‘ ì£¼ì…
    train_env_fn = lambda: TradingEnv(
        train_df, 
        obs_means=obs_means, 
        obs_stds=obs_stds, 
        window_size=window_size, 
        episode_length=episode_length
    )
    train_env = DummyVecEnv([train_env_fn]) 
    
    # --- ê²€ì¦ í™˜ê²½ ---
    # âš ï¸ [ì¤‘ìš”] ê²€ì¦ í™˜ê²½ì—ë„ 'í›ˆë ¨ ë°ì´í„°ì˜ í†µê³„'ë¥¼ ë™ì¼í•˜ê²Œ ì£¼ì…
    eval_env_fn = lambda: TradingEnv(
        eval_df, 
        obs_means=obs_means, 
        obs_stds=obs_stds, 
        window_size=window_size, 
        episode_length=len(eval_df) # ê²€ì¦ì€ ì „ì²´ ê¸°ê°„ì„ í•œ ì—í”¼ì†Œë“œë¡œ
    )
    eval_env = DummyVecEnv([eval_env_fn]) 
    
    print("í™˜ê²½ ìƒì„± ì™„ë£Œ. (VecNormalize ëŒ€ì‹  í™˜ê²½ ë‚´ë¶€ ì •ê·œí™” ë° í†µê³„ ì£¼ì… ì‚¬ìš©)")

    # ===== 5. ì½œë°±(Callback) ì„¤ì • =====
    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=10, min_evals=20, verbose=1)

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=best_model_save_path,
        log_path=log_dir,
        eval_freq=model_params['n_steps'] * 10,
        n_eval_episodes=1, 
        deterministic=True,
        render=False,
        callback_on_new_best=stop_train_callback 
    )

    # ===== 6. ëª¨ë¸ ìƒì„± ë° í•™ìŠµ =====
    model = RecurrentPPO(env=train_env, **model_params)
    
    print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    model.learn(
        total_timesteps=total_timesteps,
        callback=eval_callback 
    )

    # ===== 7. ìµœì¢… ëª¨ë¸ ì €ì¥ =====
    print("âœ… í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.")
    model.save(final_model_save_path)
    
    print(f"ìµœì¢… ëª¨ë¸: '{final_model_save_path}.zip'")
    print(f"í›ˆë ¨ í†µê³„: '{stats_save_path}'")


if __name__ == '__main__': 
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./1_model", exist_ok=True)
    os.makedirs("./0_logs/recurrent_ppo2", exist_ok=True)
    os.makedirs("./0_data", exist_ok=True) 
    
    train_agent()