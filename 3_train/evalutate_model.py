import os
import pandas as pd
import numpy as np
import pickle 
from stable_baselines3.common.vec_env import DummyVecEnv
from sb3_contrib import RecurrentPPO
from env2 import TradingEnv # TradingEnv í´ë˜ìŠ¤ê°€ ìˆëŠ” ëª¨ë“ˆì„ ì„í¬íŠ¸

# --- ê²½ë¡œ ì„¤ì • (í•™ìŠµ ì½”ë“œì™€ ë™ì¼) ---
data_path = "0_data/updated.csv"
stats_save_path = "./1_model/obs_stats_btc2.pkl"
# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
model_path = "./1_model/final_model_rnn_ppo.zip" 
# "./1_model/best_model/best_model.zip"

# --- í•™ìŠµ íŒŒë¼ë¯¸í„° (í•™ìŠµ ì½”ë“œì™€ ë™ì¼) ---
train_test_split_ratio = 0.8
window_size = 10 
OBS_COLS = ['60_BB_Width', '30_VPT', '30_ADI', 
            'day_of_week', '30_OBV', '30_to_60_Close_ratio', 
            '30_BB_High', '30_ATR', '60_ADX']

def evaluate_agent():
    # 1. ë°ì´í„° ë¡œë“œ ë° ë¶„ë¦¬ (í•™ìŠµ ì‹œì ê³¼ ë™ì¼í•˜ê²Œ)
    df = pd.read_csv(data_path, parse_dates=['datetime'], index_col='datetime')
    df = df.dropna(subset=OBS_COLS + ['Close']) 
    df = df.reset_index(drop=True) 

    split_point = int(len(df) * train_test_split_ratio)
    eval_df = df[split_point:].copy() # ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì‚¬ìš©

    print(f"í‰ê°€ ë°ì´í„° í¬ê¸°: {len(eval_df)}")

    # 2. ì •ê·œí™” í†µê³„ ë¡œë“œ
    with open(stats_save_path, 'rb') as f:
        stats = pickle.load(f)
    
    obs_means = pd.Series(stats['means'])
    obs_stds = pd.Series(stats['stds'])
    print("ì •ê·œí™” í†µê³„ ë¡œë“œ ì™„ë£Œ.")

    # 3. í‰ê°€ í™˜ê²½ ìƒì„±
    # âš ï¸ [ì¤‘ìš”] í‰ê°€ ê¸°ê°„ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œë¡œ ì„¤ì • (episode_length=len(eval_df))
    eval_env_fn = lambda: TradingEnv(
        eval_df, 
        obs_means=obs_means, 
        obs_stds=obs_stds, 
        window_size=window_size, 
        episode_length=len(eval_df)
    )
    eval_env = DummyVecEnv([eval_env_fn]) 
    print("í‰ê°€ í™˜ê²½ ìƒì„± ì™„ë£Œ.")
    
    # 4. ëª¨ë¸ ë¡œë“œ
    try:
        model = RecurrentPPO.load(model_path, env=eval_env)
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    except FileNotFoundError:
        print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    # 5. í‰ê°€ ì‹¤í–‰ (ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìœ¼ë¯€ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤)
    initial_close = eval_df['Close'].iloc[window_size] 
    final_close = eval_df['Close'].iloc[-1]
    
    print("ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

    # 6. í‰ê°€ ì‹¤í–‰ (í•„ìˆ˜ ì¸ìˆ˜ì¸ initial_closeì™€ final_closeë¥¼ ì „ë‹¬)
    evaluate(model, eval_env, initial_close, final_close)
    
def evaluate(model, env, initial_close, final_close, num_episodes=1):
    """
    ì—ì´ì „íŠ¸ë¥¼ í‰ê°€í•˜ê³  ì—í”¼ì†Œë“œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë©° B&H ë²¤ì¹˜ë§ˆí¬ë¥¼ ì¶œë ¥
    """
    episode_rewards = []
    
    # Buy & Hold (B&H) ìˆ˜ìµë¥  ê³„ì‚°
    b_and_h_return = (final_close / initial_close) - 1

    for _ in range(num_episodes):
        obs = env.reset()
        done = False
        total_reward = 0
        
        lstm_states = None 
        
        actions = []
        rewards = []
        
        # âš ï¸ [ì¶”ê°€] í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        portfolio_values = []
        
        while not done:
            action, lstm_states = model.predict(obs, state=lstm_states, deterministic=True)
            
            # 4ê°œ í•­ëª©ì„ ë°›ìŠµë‹ˆë‹¤. (obs, reward, done, info)
            obs, reward, done_vec, info = env.step(action)
            
            done = done_vec[0] 
            total_reward += reward[0]
            
            # âš ï¸ [ì¶”ê°€] TradingEnvì˜ infoì—ì„œ í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥
            if 'current_value' in info[0]:
                 portfolio_values.append(info[0]['current_value'])
                 
            actions.append(action[0])
            rewards.append(reward[0])
            
        episode_rewards.append(total_reward)

    # 6. ê²°ê³¼ ë¶„ì„
    print("\n--- ğŸ’° í‰ê°€ ê²°ê³¼ ë¶„ì„ ---")
    print(f"í‰ê·  ëˆ„ì  ë³´ìƒ (Total Reward): {np.mean(episode_rewards):.2f}")
    print("-----------------------------------")
    
    if portfolio_values:
        initial_value = portfolio_values[0]
        final_value = portfolio_values[-1]
        agent_return = (final_value / initial_value) - 1
        
        # ìµœëŒ€ ë‚™í­ ê³„ì‚° (ë‹¨ìˆœí™”ëœ í˜•íƒœ, TradingEnvì˜ metricsê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì •í™•í•©ë‹ˆë‹¤)
        cumulative_max = np.maximum.accumulate(portfolio_values)
        drawdown = (cumulative_max - portfolio_values) / cumulative_max
        max_drawdown = np.max(drawdown)
        
        print(f"ğŸ“ˆ ì—ì´ì „íŠ¸ ìµœì¢… ìˆ˜ìµë¥  (Agent Return): {agent_return:.2%}")
        print(f"ğŸ“‰ ì—ì´ì „íŠ¸ ìµœëŒ€ ë‚™í­ (Max Drawdown): {max_drawdown:.2%}")
        print(f"ğŸ’° ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ (Final Value): ${final_value:,.2f}")
    
    # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥
    print("\n--- âš–ï¸ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ (Buy & Hold) ---")
    print(f"ğŸ“Š B&H ìˆ˜ìµë¥ : {b_and_h_return:.2%}")
    print("-----------------------------------")
    
    return episode_rewards
# 5. í‰ê°€ ì‹¤í–‰
if __name__ == '__main__': 
    evaluate_agent()